{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30558,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:48.726316Z",
     "iopub.execute_input": "2023-09-29T14:48:48.726563Z",
     "iopub.status.idle": "2023-09-29T14:48:49.04176Z",
     "shell.execute_reply.started": "2023-09-29T14:48:48.726539Z",
     "shell.execute_reply": "2023-09-29T14:48:49.04087Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:51.476984Z",
     "start_time": "2024-07-05T15:13:49.039440Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "# Fine-Tuning BERT for Named Entity Recognition\n\nThis notebook covers fine-tuning a pretrained BERT model for Named Entity Recognition (NER) on the CoNLL-2003 dataset. \n\nNER is a common NLP task that involves identifying and classifying key entities (people, organizations, locations etc.) in text. It is an essential step for many downstream applications.\n\nWe will use Hugging Face's implementations of BERT and Trainer to fine-tune a model to perform NER. The key steps are:\n\n1. Prepare training data and map labels  \n2. Load pretrained BERT model and tokenizer\n3. Define training arguments and trainer\n4. Fine-tune model on training data \n5. Evaluate on validation data\n\nThe trained model can extract named entities from text by encoding the text and applying the model's token classification head.\n\nThis provides a simple template for fine-tuning transformer models like BERT for sequence tagging tasks like NER. The same principles can be applied to other datasets and use cases as well.\n\nLet's get started!\n",
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:51.480761Z",
     "start_time": "2024-07-05T15:13:51.478018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = {\n",
    "    \"universalner/universal_ner\": ['ceb_gja', 'zh_gsd', 'zh_gsdsimp', 'zh_pud', 'hr_set', 'da_ddt', 'en_ewt', 'en_pud', 'de_pud', 'pt_bosque', 'pt_pud', 'ru_pud', 'sr_set', 'sk_snk', 'sv_pud', 'sv_talbanken', 'tl_trg', 'tl_ugnayan'],\n",
    "    \"DFKI-SLT/cross_ner\" : ['ai', 'conll2003', 'literature', 'music', 'politics', 'science']\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the CoNLL-2003 dataset using the 'datasets' library.\n",
    "dataset = load_dataset('conll2003')\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:49.043351Z",
     "iopub.execute_input": "2023-09-29T14:48:49.043979Z",
     "iopub.status.idle": "2023-09-29T14:48:56.560663Z",
     "shell.execute_reply.started": "2023-09-29T14:48:49.043946Z",
     "shell.execute_reply": "2023-09-29T14:48:56.55976Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:55.597111Z",
     "start_time": "2024-07-05T15:13:51.481405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# Accessing the label names from the 'ner_tags' feature.\n",
    "label_names = dataset['train'].features['ner_tags'].feature.names\n",
    "\n",
    "# Create mapping from label ID to label string name\n",
    "id2label = {k: v for k, v in enumerate(label_names)} \n",
    "\n",
    "# Create reverse mapping from label name to label ID\n",
    "label2id = {v: k for k, v in enumerate(label_names)}\n",
    "\n",
    "label_names"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:56.576303Z",
     "iopub.execute_input": "2023-09-29T14:48:56.576711Z",
     "iopub.status.idle": "2023-09-29T14:48:56.611135Z",
     "shell.execute_reply.started": "2023-09-29T14:48:56.576664Z",
     "shell.execute_reply": "2023-09-29T14:48:56.61017Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:55.601753Z",
     "start_time": "2024-07-05T15:13:55.598332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the checkpoint you want to use for the tokenizer.\n",
    "checkpoint = 'distilbert-base-cased'\n",
    "\n",
    "# Create a tokenizer instance by loading the pre-trained checkpoint.\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Create a DataCollatorForTokenClassification object\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:56.624519Z",
     "iopub.execute_input": "2023-09-29T14:48:56.625261Z",
     "iopub.status.idle": "2023-09-29T14:48:59.953903Z",
     "shell.execute_reply.started": "2023-09-29T14:48:56.625218Z",
     "shell.execute_reply": "2023-09-29T14:48:59.95299Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:55.794598Z",
     "start_time": "2024-07-05T15:13:55.602747Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "def align_target(labels, word_ids):\n",
    "    # Define a mapping from beginning (B-) labels to inside (I-) labels\n",
    "    begin2inside = {\n",
    "        1: 2,  # B-LOC -> I-LOC\n",
    "        3: 4,  # B-MISC -> I-MISC\n",
    "        5: 6,  # B-ORG -> I-ORG\n",
    "        7: 8    # B-PER -> I-PER\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store aligned labels and a variable to track the last word\n",
    "    align_labels = []\n",
    "    last_word = None\n",
    "\n",
    "    # Iterate through the word_ids\n",
    "    for word in word_ids:\n",
    "        if word is None:\n",
    "            label = -100  # Set label to -100 for None word_ids\n",
    "        elif word != last_word:\n",
    "            label = labels[word]  # Use the label corresponding to the current word_id\n",
    "        else:\n",
    "            label = labels[word]\n",
    "            # Change B- to I- if the previous word is the same\n",
    "            if label in begin2inside:\n",
    "                label = begin2inside[label]  # Map B- to I-\n",
    "\n",
    "        # Append the label to the align_labels list and update last_word\n",
    "        align_labels.append(label)\n",
    "        last_word = word\n",
    "\n",
    "    return align_labels"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:59.971656Z",
     "iopub.execute_input": "2023-09-29T14:48:59.9723Z",
     "iopub.status.idle": "2023-09-29T14:48:59.985631Z",
     "shell.execute_reply.started": "2023-09-29T14:48:59.972269Z",
     "shell.execute_reply": "2023-09-29T14:48:59.984714Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:55.799055Z",
     "start_time": "2024-07-05T15:13:55.795533Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "def tokenize_fn(batch):\n    # Tokenize the input batch\n    tokenized_inputs = tokenizer(batch['tokens'], truncation=True, is_split_into_words=True)\n\n    # Extract the labels batch from the input batch\n    labels_batch = batch['ner_tags']\n\n    # Initialize a list to store aligned targets for each example in the batch\n    aligned_targets_batch = []\n\n    # Iterate through each example and align the labels\n    for i, labels in enumerate(labels_batch):\n        # Extract the word_ids for the current example\n        word_ids = tokenized_inputs.word_ids(i)\n\n        # Use the align_target function to align the labels\n        aligned_targets_batch.append(align_target(labels, word_ids))\n\n    # Add the aligned labels to the tokenized inputs under the key \"labels\"\n    tokenized_inputs[\"labels\"] = aligned_targets_batch\n\n    # Return the tokenized inputs, including aligned labels\n    return tokenized_inputs",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:00.045727Z",
     "iopub.execute_input": "2023-09-29T14:49:00.046846Z",
     "iopub.status.idle": "2023-09-29T14:49:00.067315Z",
     "shell.execute_reply.started": "2023-09-29T14:49:00.046812Z",
     "shell.execute_reply": "2023-09-29T14:49:00.06591Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:55.802505Z",
     "start_time": "2024-07-05T15:13:55.799810Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset['train'].column_names)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:00.068438Z",
     "iopub.execute_input": "2023-09-29T14:49:00.068993Z",
     "iopub.status.idle": "2023-09-29T14:49:05.401791Z",
     "shell.execute_reply.started": "2023-09-29T14:49:00.068963Z",
     "shell.execute_reply": "2023-09-29T14:49:05.399539Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:55.818764Z",
     "start_time": "2024-07-05T15:13:55.803258Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the seqeval metric which can evaluate NER and other sequence tasks\n",
    "metric = load(\"seqeval\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:26.332376Z",
     "iopub.execute_input": "2023-09-29T14:49:26.332723Z",
     "iopub.status.idle": "2023-09-29T14:49:27.072128Z",
     "shell.execute_reply.started": "2023-09-29T14:49:26.332685Z",
     "shell.execute_reply": "2023-09-29T14:49:27.071158Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:56.667460Z",
     "start_time": "2024-07-05T15:13:55.819474Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "# Function to compute evaluation metrics from model logits and true labels\ndef compute_metrics(logits_and_labels):\n\n  # Unpack the logits and labels\n  logits, labels = logits_and_labels \n  \n  # Get predictions from the logits\n  predictions = np.argmax(logits, axis=-1)\n\n  # Remove ignored index (special tokens)\n  str_labels = [\n    [label_names[t] for t in label if t!=-100] for label in labels\n  ]\n  \n  str_preds = [\n    [label_names[p] for (p, t) in zip(prediction, label) if t != -100]\n    for prediction, label in zip(predictions, labels)\n  ]\n\n  # Compute metrics\n  results = metric.compute(predictions=str_preds, references=str_labels)\n  \n  # Extract key metrics\n  return {\n    \"precision\": results[\"overall_precision\"],\n    \"recall\": results[\"overall_recall\"], \n    \"f1\": results[\"overall_f1\"],\n    \"accuracy\": results[\"overall_accuracy\"]  \n  }",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:27.073454Z",
     "iopub.execute_input": "2023-09-29T14:49:27.074465Z",
     "iopub.status.idle": "2023-09-29T14:49:27.083093Z",
     "shell.execute_reply.started": "2023-09-29T14:49:27.074432Z",
     "shell.execute_reply": "2023-09-29T14:49:27.081728Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:56.671979Z",
     "start_time": "2024-07-05T15:13:56.668854Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "# Load pretrained token classification model from Transformers \n",
    "\n",
    "# Initialize model object with pretrained weights\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  checkpoint,\n",
    "\n",
    "  # Pass in label mappings\n",
    "  id2label=id2label,  \n",
    "  label2id=label2id\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:27.096642Z",
     "iopub.execute_input": "2023-09-29T14:49:27.097552Z",
     "iopub.status.idle": "2023-09-29T14:49:29.799171Z",
     "shell.execute_reply.started": "2023-09-29T14:49:27.097518Z",
     "shell.execute_reply": "2023-09-29T14:49:29.798275Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:56.902402Z",
     "start_time": "2024-07-05T15:13:56.672736Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "# Configure training arguments using TrainigArguments class\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  # Location to save fine-tuned model \n",
    "  output_dir = \"fine_tuned_model\",\n",
    "\n",
    "  # Evaluate each epoch\n",
    "  eval_strategy = \"epoch\",\n",
    "\n",
    "  # Learning rate for Adam optimizer\n",
    "  learning_rate = 2e-5, \n",
    "  \n",
    "  # Batch sizes for training and evaluation\n",
    "  per_device_train_batch_size = 16,\n",
    "  per_device_eval_batch_size = 16,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs = 3,\n",
    "\n",
    "  # L2 weight decay regularization\n",
    "  weight_decay = 0.01\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:29.800666Z",
     "iopub.execute_input": "2023-09-29T14:49:29.800993Z",
     "iopub.status.idle": "2023-09-29T14:49:29.839152Z",
     "shell.execute_reply.started": "2023-09-29T14:49:29.800962Z",
     "shell.execute_reply": "2023-09-29T14:49:29.838147Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:56.910439Z",
     "start_time": "2024-07-05T15:13:56.903257Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize Trainer object for model training\n",
    "\n",
    "trainer = Trainer(\n",
    "  # Model to train\n",
    "  model=model, \n",
    "  \n",
    "  # Training arguments\n",
    "  args=training_args,\n",
    "\n",
    "  # Training and validation datasets\n",
    "  train_dataset=tokenized_dataset[\"train\"],\n",
    "  eval_dataset=tokenized_dataset[\"validation\"],\n",
    "\n",
    "  # Tokenizer\n",
    "  tokenizer=tokenizer,\n",
    "\n",
    "  # Custom metric function\n",
    "  compute_metrics=compute_metrics,\n",
    "\n",
    "  # Data collator\n",
    "  data_collator=data_collator \n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:29.841318Z",
     "iopub.execute_input": "2023-09-29T14:49:29.841865Z",
     "iopub.status.idle": "2023-09-29T14:49:35.2154Z",
     "shell.execute_reply.started": "2023-09-29T14:49:29.841833Z",
     "shell.execute_reply": "2023-09-29T14:49:35.214479Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:13:56.926828Z",
     "start_time": "2024-07-05T15:13:56.911211Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "trainer.train()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:35.216515Z",
     "iopub.execute_input": "2023-09-29T14:49:35.216818Z",
     "iopub.status.idle": "2023-09-29T14:55:15.112378Z",
     "shell.execute_reply.started": "2023-09-29T14:49:35.216789Z",
     "shell.execute_reply": "2023-09-29T14:55:15.11083Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:47:41.196751Z",
     "start_time": "2024-07-05T15:13:56.927518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2634/2634 33:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.091448</td>\n",
       "      <td>0.857212</td>\n",
       "      <td>0.890104</td>\n",
       "      <td>0.873349</td>\n",
       "      <td>0.971611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.070413</td>\n",
       "      <td>0.908386</td>\n",
       "      <td>0.926119</td>\n",
       "      <td>0.917167</td>\n",
       "      <td>0.981147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.068798</td>\n",
       "      <td>0.910573</td>\n",
       "      <td>0.930495</td>\n",
       "      <td>0.920426</td>\n",
       "      <td>0.981780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2634, training_loss=0.09485431865208423, metrics={'train_runtime': 2023.8138, 'train_samples_per_second': 20.814, 'train_steps_per_second': 1.302, 'total_flos': 525319502290632.0, 'train_loss': 0.09485431865208423, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "trainer.save_model('fine_tuned_model')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:55:15.116883Z",
     "iopub.execute_input": "2023-09-29T14:55:15.117568Z",
     "iopub.status.idle": "2023-09-29T14:55:15.694566Z",
     "shell.execute_reply.started": "2023-09-29T14:55:15.11753Z",
     "shell.execute_reply": "2023-09-29T14:55:15.693454Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:47:41.430091Z",
     "start_time": "2024-07-05T15:47:41.197545Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "ner = pipeline(\n",
    "    'token-classification',\n",
    "    model = 'fine_tuned_model',\n",
    "    aggregation_strategy = 'simple' , \n",
    "    device = \"cpu\" \n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:55:15.69585Z",
     "iopub.execute_input": "2023-09-29T14:55:15.696379Z",
     "iopub.status.idle": "2023-09-29T14:55:18.335813Z",
     "shell.execute_reply.started": "2023-09-29T14:55:15.696341Z",
     "shell.execute_reply": "2023-09-29T14:55:18.334522Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:49:28.158322Z",
     "start_time": "2024-07-05T15:49:28.092862Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "ner('Apple Inc. is planning to open a new store in San Francisco, California.')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T15:05:22.749477Z",
     "iopub.execute_input": "2023-09-29T15:05:22.749813Z",
     "iopub.status.idle": "2023-09-29T15:05:22.773415Z",
     "shell.execute_reply.started": "2023-09-29T15:05:22.749787Z",
     "shell.execute_reply": "2023-09-29T15:05:22.772205Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T15:49:29.896835Z",
     "start_time": "2024-07-05T15:49:29.874375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9944388,\n",
       "  'word': 'Apple Inc.',\n",
       "  'start': 0,\n",
       "  'end': 10},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9967184,\n",
       "  'word': 'San Francisco',\n",
       "  'start': 46,\n",
       "  'end': 59},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9979243,\n",
       "  'word': 'California',\n",
       "  'start': 61,\n",
       "  'end': 71}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
