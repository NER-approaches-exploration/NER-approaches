{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30558,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:48.726316Z",
     "iopub.execute_input": "2023-09-29T14:48:48.726563Z",
     "iopub.status.idle": "2023-09-29T14:48:49.04176Z",
     "shell.execute_reply.started": "2023-09-29T14:48:48.726539Z",
     "shell.execute_reply": "2023-09-29T14:48:49.04087Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:40.559701Z",
     "start_time": "2024-07-04T11:08:39.464415Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "# Fine-Tuning BERT for Named Entity Recognition\n\nThis notebook covers fine-tuning a pretrained BERT model for Named Entity Recognition (NER) on the CoNLL-2003 dataset. \n\nNER is a common NLP task that involves identifying and classifying key entities (people, organizations, locations etc.) in text. It is an essential step for many downstream applications.\n\nWe will use Hugging Face's implementations of BERT and Trainer to fine-tune a model to perform NER. The key steps are:\n\n1. Prepare training data and map labels  \n2. Load pretrained BERT model and tokenizer\n3. Define training arguments and trainer\n4. Fine-tune model on training data \n5. Evaluate on validation data\n\nThe trained model can extract named entities from text by encoding the text and applying the model's token classification head.\n\nThis provides a simple template for fine-tuning transformer models like BERT for sequence tagging tasks like NER. The same principles can be applied to other datasets and use cases as well.\n\nLet's get started!\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the CoNLL-2003 dataset using the 'datasets' library.\n",
    "dataset = load_dataset('conll2003')\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:49.043351Z",
     "iopub.execute_input": "2023-09-29T14:48:49.043979Z",
     "iopub.status.idle": "2023-09-29T14:48:56.560663Z",
     "shell.execute_reply.started": "2023-09-29T14:48:49.043946Z",
     "shell.execute_reply": "2023-09-29T14:48:56.55976Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:47.571964Z",
     "start_time": "2024-07-04T11:08:40.560868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "# Printing the features of the training dataset.\nprint(dataset['train'].features)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:56.562089Z",
     "iopub.execute_input": "2023-09-29T14:48:56.562791Z",
     "iopub.status.idle": "2023-09-29T14:48:56.572558Z",
     "shell.execute_reply.started": "2023-09-29T14:48:56.562758Z",
     "shell.execute_reply": "2023-09-29T14:48:56.571303Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:47.575992Z",
     "start_time": "2024-07-04T11:08:47.573296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None), 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "# Accessing the label names from the 'ner_tags' feature.\nlabel_names = dataset['train'].features['ner_tags'].feature.names\n\nlabel_names",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:56.576303Z",
     "iopub.execute_input": "2023-09-29T14:48:56.576711Z",
     "iopub.status.idle": "2023-09-29T14:48:56.611135Z",
     "shell.execute_reply.started": "2023-09-29T14:48:56.576664Z",
     "shell.execute_reply": "2023-09-29T14:48:56.61017Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:47.580367Z",
     "start_time": "2024-07-04T11:08:47.577268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "dataset['train'][0]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:56.613273Z",
     "iopub.execute_input": "2023-09-29T14:48:56.613961Z",
     "iopub.status.idle": "2023-09-29T14:48:56.623275Z",
     "shell.execute_reply.started": "2023-09-29T14:48:56.613931Z",
     "shell.execute_reply": "2023-09-29T14:48:56.622127Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:47.585331Z",
     "start_time": "2024-07-04T11:08:47.581351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer\n\n# Define the checkpoint you want to use for the tokenizer.\ncheckpoint = 'distilbert-base-cased'\n\n# Create a tokenizer instance by loading the pre-trained checkpoint.\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:56.624519Z",
     "iopub.execute_input": "2023-09-29T14:48:56.625261Z",
     "iopub.status.idle": "2023-09-29T14:48:59.953903Z",
     "shell.execute_reply.started": "2023-09-29T14:48:56.625218Z",
     "shell.execute_reply": "2023-09-29T14:48:59.95299Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:49.881505Z",
     "start_time": "2024-07-04T11:08:47.586183Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "# Tokenize the first training example from the dataset \ntoken = tokenizer(dataset['train'][0]['tokens'], is_split_into_words = True)\n\n# Print the tokenizer object, the tokenized tokens, and the word IDs\nprint(token, '\\n--------------------------------------------------------------------------------------\\n', \n      token.tokens(),'\\n--------------------------------------------------------------------------------------\\n',\n      token.word_ids())",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:59.954877Z",
     "iopub.execute_input": "2023-09-29T14:48:59.955384Z",
     "iopub.status.idle": "2023-09-29T14:48:59.970195Z",
     "shell.execute_reply.started": "2023-09-29T14:48:59.955352Z",
     "shell.execute_reply": "2023-09-29T14:48:59.969352Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:49.886175Z",
     "start_time": "2024-07-04T11:08:49.882455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n",
      "--------------------------------------------------------------------------------------\n",
      " ['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]'] \n",
      "--------------------------------------------------------------------------------------\n",
      " [None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "def align_target(labels, word_ids):\n    # Define a mapping from beginning (B-) labels to inside (I-) labels\n    begin2inside = {\n        1: 2,  # B-LOC -> I-LOC\n        3: 4,  # B-MISC -> I-MISC\n        5: 6,  # B-ORG -> I-ORG\n        7: 8    # B-PER -> I-PER\n    }\n\n    # Initialize an empty list to store aligned labels and a variable to track the last word\n    align_labels = []\n    last_word = None\n\n    # Iterate through the word_ids\n    for word in word_ids:\n        if word is None:\n            label = -100  # Set label to -100 for None word_ids\n        elif word != last_word:\n            label = labels[word]  # Use the label corresponding to the current word_id\n        else:\n            label = labels[word]\n            # Change B- to I- if the previous word is the same\n            if label in begin2inside:\n                label = begin2inside[label]  # Map B- to I-\n\n        # Append the label to the align_labels list and update last_word\n        align_labels.append(label)\n        last_word = word\n\n    return align_labels",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:59.971656Z",
     "iopub.execute_input": "2023-09-29T14:48:59.9723Z",
     "iopub.status.idle": "2023-09-29T14:48:59.985631Z",
     "shell.execute_reply.started": "2023-09-29T14:48:59.972269Z",
     "shell.execute_reply": "2023-09-29T14:48:59.984714Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:49.889991Z",
     "start_time": "2024-07-04T11:08:49.886879Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "# Extract labels and word_ids\nlabels = dataset['train'][0]['ner_tags']\nword_ids = token.word_ids()\n\n# Use the align_target function to align labels\naligned_target = align_target(labels, word_ids)\n\n# Print tokenized tokens, original labels, and aligned labels\nprint(token.tokens(), '\\n--------------------------------------------------------------------------------------\\n',\n      labels, '\\n--------------------------------------------------------------------------------------\\n',\n      aligned_target)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:59.98726Z",
     "iopub.execute_input": "2023-09-29T14:48:59.987899Z",
     "iopub.status.idle": "2023-09-29T14:48:59.997327Z",
     "shell.execute_reply.started": "2023-09-29T14:48:59.987869Z",
     "shell.execute_reply": "2023-09-29T14:48:59.996382Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:49.893816Z",
     "start_time": "2024-07-04T11:08:49.890741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]'] \n",
      "--------------------------------------------------------------------------------------\n",
      " [3, 0, 7, 0, 0, 0, 7, 0, 0] \n",
      "--------------------------------------------------------------------------------------\n",
      " [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "# Create a list of aligned labels using label names\naligned_labels = [label_names[t] if t >= 0 else None for t in aligned_target]\n\n# Loop through tokens and aligned labels and print them\nfor x, y in zip(token.tokens(), aligned_labels):\n    print(f\"{x}\\t{y}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:00.005332Z",
     "iopub.execute_input": "2023-09-29T14:49:00.006809Z",
     "iopub.status.idle": "2023-09-29T14:49:00.02788Z",
     "shell.execute_reply.started": "2023-09-29T14:49:00.006777Z",
     "shell.execute_reply": "2023-09-29T14:49:00.026341Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:49.898241Z",
     "start_time": "2024-07-04T11:08:49.895590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\tNone\n",
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "la\tO\n",
      "##mb\tO\n",
      ".\tO\n",
      "[SEP]\tNone\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "# Define fake input data\nwords = ['[CLS]', 'Ger', '##man', 'call', 'to', 'Micro', '##so', '##ft', '[SEP]']\nword_ids = [None, 0, 0, 1, 2, 3, 3, 3, None]\nlabels = [7, 0, 0, 3, 4]\n\n# Use the align_target function to align labels\naligned_target = align_target(labels, word_ids)\n\n# Create a list of aligned labels using label names\naligned_labels = [label_names[t] if t >= 0 else None for t in aligned_target]\n\n# Loop through words and aligned labels and print them\nfor x, y in zip(words, aligned_labels):\n    print(f\"{x}\\t{y}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:00.028767Z",
     "iopub.execute_input": "2023-09-29T14:49:00.029813Z",
     "iopub.status.idle": "2023-09-29T14:49:00.042949Z",
     "shell.execute_reply.started": "2023-09-29T14:49:00.029571Z",
     "shell.execute_reply": "2023-09-29T14:49:00.042045Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:49.902590Z",
     "start_time": "2024-07-04T11:08:49.899243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\tNone\n",
      "Ger\tB-MISC\n",
      "##man\tI-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "Micro\tB-ORG\n",
      "##so\tI-ORG\n",
      "##ft\tI-ORG\n",
      "[SEP]\tNone\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "def tokenize_fn(batch):\n    # Tokenize the input batch\n    tokenized_inputs = tokenizer(batch['tokens'], truncation=True, is_split_into_words=True)\n\n    # Extract the labels batch from the input batch\n    labels_batch = batch['ner_tags']\n\n    # Initialize a list to store aligned targets for each example in the batch\n    aligned_targets_batch = []\n\n    # Iterate through each example and align the labels\n    for i, labels in enumerate(labels_batch):\n        # Extract the word_ids for the current example\n        word_ids = tokenized_inputs.word_ids(i)\n\n        # Use the align_target function to align the labels\n        aligned_targets_batch.append(align_target(labels, word_ids))\n\n    # Add the aligned labels to the tokenized inputs under the key \"labels\"\n    tokenized_inputs[\"labels\"] = aligned_targets_batch\n\n    # Return the tokenized inputs, including aligned labels\n    return tokenized_inputs",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:00.045727Z",
     "iopub.execute_input": "2023-09-29T14:49:00.046846Z",
     "iopub.status.idle": "2023-09-29T14:49:00.067315Z",
     "shell.execute_reply.started": "2023-09-29T14:49:00.046812Z",
     "shell.execute_reply": "2023-09-29T14:49:00.06591Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:49.906328Z",
     "start_time": "2024-07-04T11:08:49.903355Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset['train'].column_names)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:00.068438Z",
     "iopub.execute_input": "2023-09-29T14:49:00.068993Z",
     "iopub.status.idle": "2023-09-29T14:49:05.401791Z",
     "shell.execute_reply.started": "2023-09-29T14:49:00.068963Z",
     "shell.execute_reply": "2023-09-29T14:49:05.399539Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:49.934640Z",
     "start_time": "2024-07-04T11:08:49.907210Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Create a DataCollatorForTokenClassification object\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# Testing data using the data collator\n",
    "batch = data_collator([tokenized_dataset['train'][i] for i in range(2)])\n",
    "\n",
    "# Display the resulting batch\n",
    "batch"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:05.406457Z",
     "iopub.execute_input": "2023-09-29T14:49:05.408766Z",
     "iopub.status.idle": "2023-09-29T14:49:14.625961Z",
     "shell.execute_reply.started": "2023-09-29T14:49:05.408702Z",
     "shell.execute_reply": "2023-09-29T14:49:14.624948Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:50.761288Z",
     "start_time": "2024-07-04T11:08:49.935581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495, 12913,\n",
       "           119,   102],\n",
       "        [  101,  1943, 14428,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "# Install the seqeval library for evaluating sequence tasks\n",
    "# !pip install seqeval ; "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:14.627275Z",
     "iopub.execute_input": "2023-09-29T14:49:14.628496Z",
     "iopub.status.idle": "2023-09-29T14:49:26.330389Z",
     "shell.execute_reply.started": "2023-09-29T14:49:14.62846Z",
     "shell.execute_reply": "2023-09-29T14:49:26.3293Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:50.764194Z",
     "start_time": "2024-07-04T11:08:50.762235Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "# Import the seqeval metric from Hugging Face's datasets library\n",
    "# from datasets import load_metric  \n",
    "from evaluate import load\n",
    "\n",
    "# Load the seqeval metric which can evaluate NER and other sequence tasks\n",
    "metric = load(\"seqeval\")\n",
    "\n",
    "# Example usage: compute metric on a sample predictions and reference list \n",
    "# predictions and references should be a list of lists containing predicted and true token labels\n",
    "\n",
    "# List of List Input  \n",
    "# metric.compute(predictions = [['O' , 'B-ORG' , 'I-ORG']], \n",
    "#                references = [['O' , 'B-ORG' , 'I-ORG']])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:26.332376Z",
     "iopub.execute_input": "2023-09-29T14:49:26.332723Z",
     "iopub.status.idle": "2023-09-29T14:49:27.072128Z",
     "shell.execute_reply.started": "2023-09-29T14:49:26.332685Z",
     "shell.execute_reply": "2023-09-29T14:49:27.071158Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:54.100849Z",
     "start_time": "2024-07-04T11:08:50.765026Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "# Function to compute evaluation metrics from model logits and true labels\ndef compute_metrics(logits_and_labels):\n\n  # Unpack the logits and labels\n  logits, labels = logits_and_labels \n  \n  # Get predictions from the logits\n  predictions = np.argmax(logits, axis=-1)\n\n  # Remove ignored index (special tokens)\n  str_labels = [\n    [label_names[t] for t in label if t!=-100] for label in labels\n  ]\n  \n  str_preds = [\n    [label_names[p] for (p, t) in zip(prediction, label) if t != -100]\n    for prediction, label in zip(predictions, labels)\n  ]\n\n  # Compute metrics\n  results = metric.compute(predictions=str_preds, references=str_labels)\n  \n  # Extract key metrics\n  return {\n    \"precision\": results[\"overall_precision\"],\n    \"recall\": results[\"overall_recall\"], \n    \"f1\": results[\"overall_f1\"],\n    \"accuracy\": results[\"overall_accuracy\"]  \n  }",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:27.073454Z",
     "iopub.execute_input": "2023-09-29T14:49:27.074465Z",
     "iopub.status.idle": "2023-09-29T14:49:27.083093Z",
     "shell.execute_reply.started": "2023-09-29T14:49:27.074432Z",
     "shell.execute_reply": "2023-09-29T14:49:27.081728Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:54.105217Z",
     "start_time": "2024-07-04T11:08:54.101642Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "# Create mapping from label ID to label string name\nid2label = {k: v for k, v in enumerate(label_names)} \n\n# Create reverse mapping from label name to label ID\nlabel2id = {v: k for k, v in enumerate(label_names)}\n\nprint(id2label , '\\n--------------------\\n' , label2id)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:27.084455Z",
     "iopub.execute_input": "2023-09-29T14:49:27.084786Z",
     "iopub.status.idle": "2023-09-29T14:49:27.094943Z",
     "shell.execute_reply.started": "2023-09-29T14:49:27.084755Z",
     "shell.execute_reply": "2023-09-29T14:49:27.094033Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:54.108576Z",
     "start_time": "2024-07-04T11:08:54.105892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'} \n",
      "--------------------\n",
      " {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "# Load pretrained token classification model from Transformers \nfrom transformers import AutoModelForTokenClassification\n\n# Initialize model object with pretrained weights\nmodel = AutoModelForTokenClassification.from_pretrained(\n  checkpoint,\n\n  # Pass in label mappings\n  id2label=id2label,  \n  label2id=label2id\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:27.096642Z",
     "iopub.execute_input": "2023-09-29T14:49:27.097552Z",
     "iopub.status.idle": "2023-09-29T14:49:29.799171Z",
     "shell.execute_reply.started": "2023-09-29T14:49:27.097518Z",
     "shell.execute_reply": "2023-09-29T14:49:29.798275Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:54.439689Z",
     "start_time": "2024-07-04T11:08:54.109373Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "# Configure training arguments using TrainigArguments class\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  # Location to save fine-tuned model \n",
    "  output_dir = \"fine_tuned_model\",\n",
    "\n",
    "  # Evaluate each epoch\n",
    "  eval_strategy = \"epoch\",\n",
    "\n",
    "  # Learning rate for Adam optimizer\n",
    "  learning_rate = 2e-5, \n",
    "  \n",
    "  # Batch sizes for training and evaluation\n",
    "  per_device_train_batch_size = 16,\n",
    "  per_device_eval_batch_size = 16,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs = 3,\n",
    "\n",
    "  # L2 weight decay regularization\n",
    "  weight_decay = 0.01\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:29.800666Z",
     "iopub.execute_input": "2023-09-29T14:49:29.800993Z",
     "iopub.status.idle": "2023-09-29T14:49:29.839152Z",
     "shell.execute_reply.started": "2023-09-29T14:49:29.800962Z",
     "shell.execute_reply": "2023-09-29T14:49:29.838147Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:54.461919Z",
     "start_time": "2024-07-04T11:08:54.440605Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "# Initialize Trainer object for model training\nfrom transformers import Trainer\n\ntrainer = Trainer(\n  # Model to train\n  model=model, \n  \n  # Training arguments\n  args=training_args,\n\n  # Training and validation datasets\n  train_dataset=tokenized_dataset[\"train\"],\n  eval_dataset=tokenized_dataset[\"validation\"],\n\n  # Tokenizer\n  tokenizer=tokenizer,\n\n  # Custom metric function\n  compute_metrics=compute_metrics,\n\n  # Data collator\n  data_collator=data_collator \n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:29.841318Z",
     "iopub.execute_input": "2023-09-29T14:49:29.841865Z",
     "iopub.status.idle": "2023-09-29T14:49:35.2154Z",
     "shell.execute_reply.started": "2023-09-29T14:49:29.841833Z",
     "shell.execute_reply": "2023-09-29T14:49:35.214479Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:08:54.502946Z",
     "start_time": "2024-07-04T11:08:54.462885Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": "trainer.train()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:35.216515Z",
     "iopub.execute_input": "2023-09-29T14:49:35.216818Z",
     "iopub.status.idle": "2023-09-29T14:55:15.112378Z",
     "shell.execute_reply.started": "2023-09-29T14:49:35.216789Z",
     "shell.execute_reply": "2023-09-29T14:55:15.11083Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T11:11:13.351052Z",
     "start_time": "2024-07-04T11:08:54.503718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 170/2634 02:15 < 33:14, 1.24 it/s, Epoch 0.19/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/transformers/trainer.py:1932\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1930\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   1931\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1932\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1933\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1934\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1935\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1936\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1937\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/transformers/trainer.py:2268\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2265\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   2267\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 2268\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2271\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2272\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2273\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2274\u001B[0m ):\n\u001B[1;32m   2275\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2276\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/transformers/trainer.py:3307\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   3304\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   3306\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 3307\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3309\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m   3311\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/transformers/trainer.py:3338\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   3336\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3337\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 3338\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3339\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   3340\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   3341\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:1221\u001B[0m, in \u001B[0;36mDistilBertForTokenClassification.forward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1215\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1216\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1217\u001B[0m \u001B[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001B[39;00m\n\u001B[1;32m   1218\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1219\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1221\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistilbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1222\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1224\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1225\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1226\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1227\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1228\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1229\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1231\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1233\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(sequence_output)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:810\u001B[0m, in \u001B[0;36mDistilBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    808\u001B[0m         attention_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(input_shape, device\u001B[38;5;241m=\u001B[39mdevice)  \u001B[38;5;66;03m# (bs, seq_length)\u001B[39;00m\n\u001B[0;32m--> 810\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    811\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    813\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    814\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    815\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    816\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    817\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:571\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    563\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    564\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    565\u001B[0m         hidden_state,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    568\u001B[0m         output_attentions,\n\u001B[1;32m    569\u001B[0m     )\n\u001B[1;32m    570\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 571\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    572\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    573\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    574\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    575\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    576\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    578\u001B[0m hidden_state \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    580\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:515\u001B[0m, in \u001B[0;36mTransformerBlock.forward\u001B[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001B[0m\n\u001B[1;32m    512\u001B[0m sa_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msa_layer_norm(sa_output \u001B[38;5;241m+\u001B[39m x)  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[1;32m    514\u001B[0m \u001B[38;5;66;03m# Feed Forward Network\u001B[39;00m\n\u001B[0;32m--> 515\u001B[0m ffn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mffn\u001B[49m\u001B[43m(\u001B[49m\u001B[43msa_output\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[1;32m    516\u001B[0m ffn_output: torch\u001B[38;5;241m.\u001B[39mTensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_layer_norm(ffn_output \u001B[38;5;241m+\u001B[39m sa_output)  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[1;32m    518\u001B[0m output \u001B[38;5;241m=\u001B[39m (ffn_output,)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:450\u001B[0m, in \u001B[0;36mFFN.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m--> 450\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mff_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/transformers/pytorch_utils.py:238\u001B[0m, in \u001B[0;36mapply_chunking_to_forward\u001B[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_chunks, dim\u001B[38;5;241m=\u001B[39mchunk_dim)\n\u001B[0;32m--> 238\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:455\u001B[0m, in \u001B[0;36mFFN.ff_chunk\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    453\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlin1(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m    454\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(x)\n\u001B[0;32m--> 455\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlin2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    456\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/bert-JeHI3B9r-py3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": "trainer.save_model('fine_tuned_model')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:55:15.116883Z",
     "iopub.execute_input": "2023-09-29T14:55:15.117568Z",
     "iopub.status.idle": "2023-09-29T14:55:15.694566Z",
     "shell.execute_reply.started": "2023-09-29T14:55:15.11753Z",
     "shell.execute_reply": "2023-09-29T14:55:15.693454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\n",
    "    'token-classification',\n",
    "    model = 'fine_tuned_model',\n",
    "    aggregation_strategy = 'simple' , \n",
    "    device = \"cpu\" \n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:55:15.69585Z",
     "iopub.execute_input": "2023-09-29T14:55:15.696379Z",
     "iopub.status.idle": "2023-09-29T14:55:18.335813Z",
     "shell.execute_reply.started": "2023-09-29T14:55:15.696341Z",
     "shell.execute_reply": "2023-09-29T14:55:18.334522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "ner('Apple Inc. is planning to open a new store in San Francisco, California.')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T15:05:22.749477Z",
     "iopub.execute_input": "2023-09-29T15:05:22.749813Z",
     "iopub.status.idle": "2023-09-29T15:05:22.773415Z",
     "shell.execute_reply.started": "2023-09-29T15:05:22.749787Z",
     "shell.execute_reply": "2023-09-29T15:05:22.772205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
