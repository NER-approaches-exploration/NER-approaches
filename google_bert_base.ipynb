{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30558,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from evaluate import load\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:48.726316Z",
     "iopub.execute_input": "2023-09-29T14:48:48.726563Z",
     "iopub.status.idle": "2023-09-29T14:48:49.04176Z",
     "shell.execute_reply.started": "2023-09-29T14:48:48.726539Z",
     "shell.execute_reply": "2023-09-29T14:48:49.04087Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:36.268156Z",
     "start_time": "2024-07-10T15:03:30.709146Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-Tuning BERT for Named Entity Recognition\n",
    "\n",
    "This notebook covers fine-tuning a pretrained Google BERT model for Named Entity Recognition (NER) on the FactRuEval-2016, CoNLL-2003, Collection3 and BC5CDR datasets. \n",
    "\n",
    "We will use Hugging Face's implementations of BERT and Trainer to fine-tune a model to perform NER. The key steps are:\n",
    "\n",
    "1. Prepare training data and map labels  \n",
    "2. Load pretrained BERT model and tokenizer\n",
    "3. Define training arguments and trainer\n",
    "4. Fine-tune model on training data "
   ],
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:36.275691Z",
     "start_time": "2024-07-10T15:03:36.269551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_dataset_dict(dataset):\n",
    "    ids, tokens, lengths, ner_tags = [dict() for _ in range(4)]\n",
    "    for key in ['train', 'validation', 'test']:\n",
    "        data = dataset[key]['data'][0]\n",
    "        ids[key], tokens[key], lengths[key], ner_tags[key] = ([] for _ in range(4))\n",
    "        for item in data:\n",
    "            ids[key].append(item[\"id\"])\n",
    "            tokens[key].append(item[\"tokens\"])\n",
    "            lengths[key].append(item[\"length\"])\n",
    "            ner_tags[key].append(item[\"ner_tags\"])\n",
    "\n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": Dataset.from_dict({\n",
    "            \"id\": ids[\"train\"],\n",
    "            \"tokens\": tokens[\"train\"],\n",
    "            \"length\": lengths[\"train\"],\n",
    "            \"ner_tags\": ner_tags[\"train\"]\n",
    "        }), \n",
    "        \"validation\": Dataset.from_dict({\n",
    "            \"id\": ids[\"validation\"],\n",
    "            \"tokens\": tokens[\"validation\"],\n",
    "            \"length\": lengths[\"validation\"],\n",
    "            \"ner_tags\": ner_tags[\"validation\"]\n",
    "        }),\n",
    "        \"test\": Dataset.from_dict({\n",
    "            \"id\": ids[\"test\"],\n",
    "            \"tokens\": tokens[\"test\"],\n",
    "            \"length\": lengths[\"test\"],\n",
    "            \"ner_tags\": ner_tags[\"test\"]\n",
    "        })\n",
    "    }) \n",
    "    return dataset_dict"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "def get_dataset(dataset_name):\n",
    "    # Load a dataset using the 'datasets' library.\n",
    "    dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "    if dataset_name == 'gusevski/factrueval2016':\n",
    "        dataset = get_dataset_dict(dataset)\n",
    "    return dataset\n",
    "    \n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:49.043351Z",
     "iopub.execute_input": "2023-09-29T14:48:49.043979Z",
     "iopub.status.idle": "2023-09-29T14:48:56.560663Z",
     "shell.execute_reply.started": "2023-09-29T14:48:49.043946Z",
     "shell.execute_reply": "2023-09-29T14:48:56.55976Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:36.280169Z",
     "start_time": "2024-07-10T15:03:36.275691Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "def get_label_names(dataset):\n",
    "    try:\n",
    "        return dataset['train'].features['ner_tags'].feature.names        \n",
    "    except:\n",
    "        return ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:56.576303Z",
     "iopub.execute_input": "2023-09-29T14:48:56.576711Z",
     "iopub.status.idle": "2023-09-29T14:48:56.611135Z",
     "shell.execute_reply.started": "2023-09-29T14:48:56.576664Z",
     "shell.execute_reply": "2023-09-29T14:48:56.61017Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:36.284173Z",
     "start_time": "2024-07-10T15:03:36.280169Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "def align_target(labels, word_ids):\n",
    "    # Define a mapping from beginning (B-) labels to inside (I-) labels\n",
    "    begin2inside = {\n",
    "        1: 2,  # B-LOC -> I-LOC\n",
    "        3: 4,  # B-MISC -> I-MISC\n",
    "        5: 6,  # B-ORG -> I-ORG\n",
    "        7: 8    # B-PER -> I-PER\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store aligned labels and a variable to track the last word\n",
    "    align_labels = []\n",
    "    last_word = None\n",
    "\n",
    "    # Iterate through the word_ids\n",
    "    for word in word_ids:\n",
    "        if word is None:\n",
    "            label = -100  # Set label to -100 for None word_ids\n",
    "        elif word != last_word:\n",
    "            label = labels[word]  # Use the label corresponding to the current word_id\n",
    "        else:\n",
    "            label = labels[word]\n",
    "            # Change B- to I- if the previous word is the same\n",
    "            if label in begin2inside:\n",
    "                label = begin2inside[label]  # Map B- to I-\n",
    "\n",
    "        # Append the label to the align_labels list and update last_word\n",
    "        align_labels.append(label)\n",
    "        last_word = word\n",
    "\n",
    "    return align_labels"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:59.971656Z",
     "iopub.execute_input": "2023-09-29T14:48:59.9723Z",
     "iopub.status.idle": "2023-09-29T14:48:59.985631Z",
     "shell.execute_reply.started": "2023-09-29T14:48:59.972269Z",
     "shell.execute_reply": "2023-09-29T14:48:59.984714Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:36.289668Z",
     "start_time": "2024-07-10T15:03:36.285179Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_fn(batch):\n",
    "    # Tokenize the input batch\n",
    "    tokenized_inputs = tokenizer(batch['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    # Extract the labels batch from the input batch\n",
    "    labels_batch = batch['ner_tags']\n",
    "\n",
    "    # Initialize a list to store aligned targets for each example in the batch\n",
    "    aligned_targets_batch = []\n",
    "\n",
    "    # Iterate through each example and align the labels\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        # Extract the word_ids for the current example\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "\n",
    "        # Use the align_target function to align the labels\n",
    "        aligned_targets_batch.append(align_target(labels, word_ids))\n",
    "\n",
    "    # Add the aligned labels to the tokenized inputs under the key \"labels\"\n",
    "    tokenized_inputs[\"labels\"] = aligned_targets_batch\n",
    "\n",
    "    # Return the tokenized inputs, including aligned labels\n",
    "    return tokenized_inputs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:00.045727Z",
     "iopub.execute_input": "2023-09-29T14:49:00.046846Z",
     "iopub.status.idle": "2023-09-29T14:49:00.067315Z",
     "shell.execute_reply.started": "2023-09-29T14:49:00.046812Z",
     "shell.execute_reply": "2023-09-29T14:49:00.06591Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:36.294096Z",
     "start_time": "2024-07-10T15:03:36.289668Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:36.298915Z",
     "start_time": "2024-07-10T15:03:36.294096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to compute evaluation metrics from model logits and true labels\n",
    "def compute_metrics(logits, labels, label_names):\n",
    "    metric = load(\"seqeval\")\n",
    "    # Unpack the logits and labels\n",
    "  \n",
    "    # Get predictions from the logits\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    str_labels = [\n",
    "        [label_names[t] for t in label if t!=-100] for label in labels\n",
    "    ]\n",
    "  \n",
    "    str_preds = [\n",
    "        [label_names[p] for (p, t) in zip(prediction, label) if t != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Compute metrics\n",
    "    results = metric.compute(predictions=str_preds, references=str_labels)\n",
    "  \n",
    "    # Extract key metrics\n",
    "    return results[\"overall_f1\"]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:39.027867Z",
     "start_time": "2024-07-10T15:03:36.298915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the checkpoint you want to use for the tokenizer.\n",
    "checkpoint = \"google-bert/bert-base-cased\"\n",
    "\n",
    "# Create a tokenizer instance by loading the pre-trained checkpoint.\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Create a DataCollatorForTokenClassification object\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfe8bb39b5ca4d25a6895d53ae679e7d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shipi\\PycharmProjects\\NER-approaches\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shipi\\.cache\\huggingface\\hub\\models--google-bert--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3eb1684e082b43fb83de1760e6e3a77e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "281fd63482a544c8a2317ac486f0cf66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90d8bcb0c5984793a3cbcb99348b42aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:39.146108Z",
     "start_time": "2024-07-10T15:03:39.027867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure training arguments using TrainigArguments class\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Location to save fine-tuned model \n",
    "    output_dir = f\"fine_tuned_models/{checkpoint}\",\n",
    "\n",
    "    # Evaluate each epoch\n",
    "    eval_strategy = \"epoch\",\n",
    "\n",
    "    # Learning rate for Adam optimizer\n",
    "    learning_rate = 2e-5, \n",
    "  \n",
    "    # Batch sizes for training and evaluation\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    \n",
    "    # Number of training epochs\n",
    "    num_train_epochs = 3,\n",
    "\n",
    "    # L2 weight decay regularization\n",
    "    weight_decay = 0.01\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:39.150805Z",
     "start_time": "2024-07-10T15:03:39.147135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(tokenized_dataset, label_names, id2label, label2id):\n",
    "    # Initialize model object with pretrained weights\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        \n",
    "        # Pass in label mappings\n",
    "        id2label=id2label,  \n",
    "        label2id=label2id\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "    # Model to train\n",
    "        model=model, \n",
    "      \n",
    "        # Training arguments\n",
    "        args=training_args,\n",
    "    \n",
    "        # Training and validation datasets\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    \n",
    "        # Tokenizer\n",
    "        tokenizer=tokenizer,\n",
    "    \n",
    "        # Data collator\n",
    "        data_collator=data_collator \n",
    "    )\n",
    "    trainer.train()\n",
    "    predictions = trainer.predict(tokenized_dataset['test'])\n",
    "    return compute_metrics(predictions.predictions, tokenized_dataset['test']['labels'], label_names)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:39.154240Z",
     "start_time": "2024-07-10T15:03:39.150805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = ['gusevski/factrueval2016',\n",
    "           'RCC-MSU/collection3',\n",
    "           'conll2003',\n",
    "           'ghadeermobasher/BC5CDR-Chemical-Disease']"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "print(checkpoint)\n",
    "for dataset_name in datasets:\n",
    "    dataset = get_dataset(dataset_name)\n",
    "    label_names = get_label_names(dataset)\n",
    "    id2label = {k: v for k, v in enumerate(label_names)} \n",
    "    label2id = {v: k for k, v in enumerate(label_names)}\n",
    "    tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset['train'].column_names)\n",
    "    f1 = train_model(tokenized_dataset, label_names, id2label, label2id)\n",
    "    print(f'Dataset: {dataset_name}, f1: {f1}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:55:15.116883Z",
     "iopub.execute_input": "2023-09-29T14:55:15.117568Z",
     "iopub.status.idle": "2023-09-29T14:55:15.694566Z",
     "shell.execute_reply.started": "2023-09-29T14:55:15.11753Z",
     "shell.execute_reply": "2023-09-29T14:55:15.693454Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T18:16:08.344698Z",
     "start_time": "2024-07-10T15:03:39.154240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-bert/bert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/7746 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c8569e7f19f4266ae5891cf880a5d3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2582 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a68c6e53f57547aaafffbcc95ddeaf14"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2582 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cd49ec0eb544adfb104e892559787ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba4f3a8a3a7f40e9a338f81665995666"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\shipi\\PycharmProjects\\NER-approaches\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1455' max='1455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1455/1455 1:45:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.103699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.091352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>0.077903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: gusevski/factrueval2016, f1: 0.8225751366120219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/9301 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72e7e6abc1294f42b62bd85116e56d72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2153 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34ec2e36297246668bcaf9ea257918f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1922 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b69dcc796010414daa5ae7aa9d732e7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1746' max='1746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1746/1746 1:05:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.198400</td>\n",
       "      <td>0.072591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.056117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.053088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: RCC-MSU/collection3, f1: 0.8496607546720628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1cd6127750242059238f315b9079e3c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0161974472345bb95e56c6cdc31d2e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a20ffa198c142eb8e2433b8eca2a9ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2634/2634 08:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.063826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.060399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.057518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: conll2003, f1: 0.8979627372453421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4561 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "318a9cbc70b84394a1bac3efd32f52e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4582 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f89ae18de614ee98416760ff317c169"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4798 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f417f1c0d43d41f88e43d81d26539942"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [858/858 04:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.132229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.152535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ghadeermobasher/BC5CDR-Chemical-Disease, f1: 0.8005596294866847\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ]
}
