{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30558,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from evaluate import load\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:48.726316Z",
     "iopub.execute_input": "2023-09-29T14:48:48.726563Z",
     "iopub.status.idle": "2023-09-29T14:48:49.04176Z",
     "shell.execute_reply.started": "2023-09-29T14:48:48.726539Z",
     "shell.execute_reply": "2023-09-29T14:48:49.04087Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T11:06:15.623683Z",
     "start_time": "2024-07-10T11:06:08.544481Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-Tuning BERT for Named Entity Recognition\n",
    "\n",
    "This notebook covers fine-tuning a pretrained Google BERT multilingual model for Named Entity Recognition (NER) on the FactRuEval-2016, CoNLL-2003, Collection3 and BC5CDR datasets. \n",
    "\n",
    "We will use Hugging Face's implementations of BERT and Trainer to fine-tune a model to perform NER. The key steps are:\n",
    "\n",
    "1. Prepare training data and map labels  \n",
    "2. Load pretrained BERT model and tokenizer\n",
    "3. Define training arguments and trainer\n",
    "4. Fine-tune model on training data "
   ],
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T11:06:15.630672Z",
     "start_time": "2024-07-10T11:06:15.625154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_dataset_dict(dataset):\n",
    "    ids, tokens, lengths, ner_tags = [dict() for _ in range(4)]\n",
    "    for key in ['train', 'validation', 'test']:\n",
    "        data = dataset[key]['data'][0]\n",
    "        ids[key], tokens[key], lengths[key], ner_tags[key] = ([] for _ in range(4))\n",
    "        for item in data:\n",
    "            ids[key].append(item[\"id\"])\n",
    "            tokens[key].append(item[\"tokens\"])\n",
    "            lengths[key].append(item[\"length\"])\n",
    "            ner_tags[key].append(item[\"ner_tags\"])\n",
    "\n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": Dataset.from_dict({\n",
    "            \"id\": ids[\"train\"],\n",
    "            \"tokens\": tokens[\"train\"],\n",
    "            \"length\": lengths[\"train\"],\n",
    "            \"ner_tags\": ner_tags[\"train\"]\n",
    "        }), \n",
    "        \"validation\": Dataset.from_dict({\n",
    "            \"id\": ids[\"validation\"],\n",
    "            \"tokens\": tokens[\"validation\"],\n",
    "            \"length\": lengths[\"validation\"],\n",
    "            \"ner_tags\": ner_tags[\"validation\"]\n",
    "        }),\n",
    "        \"test\": Dataset.from_dict({\n",
    "            \"id\": ids[\"test\"],\n",
    "            \"tokens\": tokens[\"test\"],\n",
    "            \"length\": lengths[\"test\"],\n",
    "            \"ner_tags\": ner_tags[\"test\"]\n",
    "        })\n",
    "    }) \n",
    "    return dataset_dict"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "def get_dataset(dataset_name):\n",
    "    # Load a dataset using the 'datasets' library.\n",
    "    dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "    if dataset_name == 'gusevski/factrueval2016':\n",
    "        dataset = get_dataset_dict(dataset)\n",
    "    return dataset\n",
    "    \n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:49.043351Z",
     "iopub.execute_input": "2023-09-29T14:48:49.043979Z",
     "iopub.status.idle": "2023-09-29T14:48:56.560663Z",
     "shell.execute_reply.started": "2023-09-29T14:48:49.043946Z",
     "shell.execute_reply": "2023-09-29T14:48:56.55976Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T11:06:15.634743Z",
     "start_time": "2024-07-10T11:06:15.631679Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "def get_label_names(dataset):\n",
    "    try:\n",
    "        return dataset['train'].features['ner_tags'].feature.names        \n",
    "    except:\n",
    "        return ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:56.576303Z",
     "iopub.execute_input": "2023-09-29T14:48:56.576711Z",
     "iopub.status.idle": "2023-09-29T14:48:56.611135Z",
     "shell.execute_reply.started": "2023-09-29T14:48:56.576664Z",
     "shell.execute_reply": "2023-09-29T14:48:56.61017Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T11:06:15.638778Z",
     "start_time": "2024-07-10T11:06:15.635755Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "def align_target(labels, word_ids):\n",
    "    # Define a mapping from beginning (B-) labels to inside (I-) labels\n",
    "    begin2inside = {\n",
    "        1: 2,  # B-LOC -> I-LOC\n",
    "        3: 4,  # B-MISC -> I-MISC\n",
    "        5: 6,  # B-ORG -> I-ORG\n",
    "        7: 8    # B-PER -> I-PER\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store aligned labels and a variable to track the last word\n",
    "    align_labels = []\n",
    "    last_word = None\n",
    "\n",
    "    # Iterate through the word_ids\n",
    "    for word in word_ids:\n",
    "        if word is None:\n",
    "            label = -100  # Set label to -100 for None word_ids\n",
    "        elif word != last_word:\n",
    "            label = labels[word]  # Use the label corresponding to the current word_id\n",
    "        else:\n",
    "            label = labels[word]\n",
    "            # Change B- to I- if the previous word is the same\n",
    "            if label in begin2inside:\n",
    "                label = begin2inside[label]  # Map B- to I-\n",
    "\n",
    "        # Append the label to the align_labels list and update last_word\n",
    "        align_labels.append(label)\n",
    "        last_word = word\n",
    "\n",
    "    return align_labels"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:48:59.971656Z",
     "iopub.execute_input": "2023-09-29T14:48:59.9723Z",
     "iopub.status.idle": "2023-09-29T14:48:59.985631Z",
     "shell.execute_reply.started": "2023-09-29T14:48:59.972269Z",
     "shell.execute_reply": "2023-09-29T14:48:59.984714Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T11:06:15.644648Z",
     "start_time": "2024-07-10T11:06:15.640788Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_fn(batch):\n",
    "    # Tokenize the input batch\n",
    "    tokenized_inputs = tokenizer(batch['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    # Extract the labels batch from the input batch\n",
    "    labels_batch = batch['ner_tags']\n",
    "\n",
    "    # Initialize a list to store aligned targets for each example in the batch\n",
    "    aligned_targets_batch = []\n",
    "\n",
    "    # Iterate through each example and align the labels\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        # Extract the word_ids for the current example\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "\n",
    "        # Use the align_target function to align the labels\n",
    "        aligned_targets_batch.append(align_target(labels, word_ids))\n",
    "\n",
    "    # Add the aligned labels to the tokenized inputs under the key \"labels\"\n",
    "    tokenized_inputs[\"labels\"] = aligned_targets_batch\n",
    "\n",
    "    # Return the tokenized inputs, including aligned labels\n",
    "    return tokenized_inputs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:49:00.045727Z",
     "iopub.execute_input": "2023-09-29T14:49:00.046846Z",
     "iopub.status.idle": "2023-09-29T14:49:00.067315Z",
     "shell.execute_reply.started": "2023-09-29T14:49:00.046812Z",
     "shell.execute_reply": "2023-09-29T14:49:00.06591Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T11:06:15.650115Z",
     "start_time": "2024-07-10T11:06:15.645606Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T11:06:15.656616Z",
     "start_time": "2024-07-10T11:06:15.651645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to compute evaluation metrics from model logits and true labels\n",
    "def compute_metrics(logits, labels, label_names):\n",
    "    metric = load(\"seqeval\")\n",
    "    # Unpack the logits and labels\n",
    "  \n",
    "    # Get predictions from the logits\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    str_labels = [\n",
    "        [label_names[t] for t in label if t!=-100] for label in labels\n",
    "    ]\n",
    "  \n",
    "    str_preds = [\n",
    "        [label_names[p] for (p, t) in zip(prediction, label) if t != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Compute metrics\n",
    "    results = metric.compute(predictions=str_preds, references=str_labels)\n",
    "  \n",
    "    # Extract key metrics\n",
    "    return results[\"overall_f1\"]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T11:06:16.346458Z",
     "start_time": "2024-07-10T11:06:15.657632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the checkpoint you want to use for the tokenizer.\n",
    "checkpoint = \"google-bert/bert-base-multilingual-cased\"\n",
    "\n",
    "# Create a tokenizer instance by loading the pre-trained checkpoint.\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Create a DataCollatorForTokenClassification object\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T11:06:17.765066Z",
     "start_time": "2024-07-10T11:06:16.346971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure training arguments using TrainigArguments class\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Location to save fine-tuned model \n",
    "    output_dir = f\"fine_tuned_models/{checkpoint}\",\n",
    "\n",
    "    # Evaluate each epoch\n",
    "    eval_strategy = \"epoch\",\n",
    "\n",
    "    # Learning rate for Adam optimizer\n",
    "    learning_rate = 2e-5, \n",
    "  \n",
    "    # Batch sizes for training and evaluation\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    \n",
    "    # Number of training epochs\n",
    "    num_train_epochs = 3,\n",
    "\n",
    "    # L2 weight decay regularization\n",
    "    weight_decay = 0.01\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T11:06:17.770148Z",
     "start_time": "2024-07-10T11:06:17.765508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(tokenized_dataset, label_names, id2label, label2id):\n",
    "    # Initialize model object with pretrained weights\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        \n",
    "        # Pass in label mappings\n",
    "        id2label=id2label,  \n",
    "        label2id=label2id\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "    # Model to train\n",
    "        model=model, \n",
    "      \n",
    "        # Training arguments\n",
    "        args=training_args,\n",
    "    \n",
    "        # Training and validation datasets\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    \n",
    "        # Tokenizer\n",
    "        tokenizer=tokenizer,\n",
    "    \n",
    "        # Data collator\n",
    "        data_collator=data_collator \n",
    "    )\n",
    "    trainer.train()\n",
    "    predictions = trainer.predict(tokenized_dataset['test'])\n",
    "    return compute_metrics(predictions.predictions, tokenized_dataset['test']['labels'], label_names)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T11:06:17.774333Z",
     "start_time": "2024-07-10T11:06:17.771673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = ['gusevski/factrueval2016',\n",
    "           'RCC-MSU/collection3',\n",
    "           'conll2003',\n",
    "           'ghadeermobasher/BC5CDR-Chemical-Disease']"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "print(checkpoint)\n",
    "for dataset_name in datasets:\n",
    "    dataset = get_dataset(dataset_name)\n",
    "    label_names = get_label_names(dataset)\n",
    "    id2label = {k: v for k, v in enumerate(label_names)} \n",
    "    label2id = {v: k for k, v in enumerate(label_names)}\n",
    "    tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset['train'].column_names)\n",
    "    f1 = train_model(tokenized_dataset, label_names, id2label, label2id)\n",
    "    print(f'Dataset: {dataset_name}, f1: {f1}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-29T14:55:15.116883Z",
     "iopub.execute_input": "2023-09-29T14:55:15.117568Z",
     "iopub.status.idle": "2023-09-29T14:55:15.694566Z",
     "shell.execute_reply.started": "2023-09-29T14:55:15.11753Z",
     "shell.execute_reply": "2023-09-29T14:55:15.693454Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T12:58:57.972592Z",
     "start_time": "2024-07-10T11:06:17.775428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-bert/bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/7746 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30fcbe32beb640ee94ed3db5a1a5d330"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2582 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4777d0968e544249292ab5a3364c89f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2582 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0d521f9e74346d199f6486daca336e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\shipi\\PycharmProjects\\NER-approaches\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1455' max='1455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1455/1455 20:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.048128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>0.037207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.036046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: gusevski/factrueval2016, f1: 0.9603628117913833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1746' max='1746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1746/1746 1:07:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.099800</td>\n",
       "      <td>0.032815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.028342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.026745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: RCC-MSU/collection3, f1: 0.9695841092489137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9726a9fec3b4ecba73a5332876ac733"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f375725cf2d4574af798e7ea84a6606"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0c9c0e868c24eb6b915bf4482091110"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2634/2634 15:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.176500</td>\n",
       "      <td>0.058879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.056605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.051256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: conll2003, f1: 0.899023369375654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4561 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25ddea24dcb04004b8507b8379d697f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4582 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1667236a88d48eebe84bf3482411641"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4798 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45ffec8125c74d9fa2b70735bd66ad04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [858/858 06:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.138583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.138396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.152658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ghadeermobasher/BC5CDR-Chemical-Disease, f1: 0.8147713950762016\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ]
}
